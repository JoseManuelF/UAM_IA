{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Práctica 4 de IA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Construcción de clasificadores en bases de datos sintéticas (1.5 puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta primera parte está adaptada de:\n",
    "# http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs, make_moons, make_circles\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation\n",
    "simple = make_blobs(n_samples=500, n_features=2,\n",
    "                    centers=[[0,0], [2.5,2.5]],\n",
    "                    random_state=1)\n",
    "X,y = make_blobs(n_samples=500, n_features=2,\n",
    "                 centers=[[0,0], [5,1]],\n",
    "                 random_state=1)\n",
    "X = X*np.matrix([[1,-2],[-20,10]])\n",
    "linearly_separable=(X,y)\n",
    "\n",
    "datasets = [simple,\n",
    "            linearly_separable,\n",
    "            make_moons(noise=0.1, random_state=0, n_samples=500),\n",
    "            make_circles(noise=0.1, factor=0.5, random_state=1,\n",
    "                         n_samples=500)\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [(\"Naive Bayes\", GaussianNB()),\n",
    "               (\"Nearest Neighbors\",\n",
    "                KNeighborsClassifier(n_neighbors=1)), # número de vecinos\n",
    "               (\"Decision Tree\",\n",
    "                DecisionTreeClassifier(criterion='entropy',\n",
    "                                       max_depth=2)), # profundidad máxima del árbol\n",
    "               (\"Logistic Regression\",\n",
    "                LogisticRegression(C=1e10,solver='lbfgs')), # C: cuanto más alto menos regularización \n",
    "               (\"Neural Network\",\n",
    "                MLPClassifier(hidden_layer_sizes=(50,),\n",
    "                              max_iter=1000,\n",
    "                              alpha=0))]\n",
    "\n",
    "from p4_IA_aux import plot_classifiers\n",
    "\n",
    "plot_classifiers(classifiers, datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listado de los parámetros de los diferentes clasificadores\n",
    "for name, clf in classifiers:\n",
    "    print(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prueba a cambiar los siguientes parámetros y observa las consecuencias en la frontera de clasificación construida:**\n",
    "\n",
    "* Número de vecinos en k-nn. ¿Por qué siempre debe ser impar cuando hay dos clases?\n",
    "* Profundidad máxima de los árboles de decisión.\n",
    "* Número de neuronas en la red neuronal y máximo número de épocas de entrenamiento. **Nota:** (50,) indica una única capa oculta con 50 neuronas. (50,10,) indica dos capas ocultas con 50 y 10 neuronas respectivamente. (50,10,20,) indica tres capas ocultas con 50, 10 y 20 neuronas respectivamente, etc."
   ]
  },
  {
   "source": [
    "Links to delete after:\n",
    "\n",
    "Question 1: https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/\n",
    "\n",
    "Question 2: https://stackoverflow.com/questions/37095085/sklearn-decisiontreeclassifier-more-depth-less-accuracy\n",
    "\n",
    "* If the number of neighbors is low, the algorithm provides a more flexible classification, leading to an irregular decision boundary. While a high number of neighbors provides a regular boundary, but it is less precise.\n",
    "The number of neighbors is odd when there are two classes because of the way the k-nn algorithm computes. As it predicts the class of the object that is currently analysing by taking the most common class among its closest neighbors, the number of neighbors has to be odd in order to prevent ties. So if k = 4, there is a possibilty that 2 of the neighbors are from one class and the other 2 from another, which is not advisable because it will choose depending on the ordering of the data.\n",
    "\n",
    "* When we set the maximum depth of the decision tree to 3 instead of 2, the difference is quite remarkable, especially in the linearly separable (from 0.59 to 0.76) and circle (from 0.73 to 0.82) datasets. As we continue increasing the maximum depth, we can observe that the simple dataset hardly changes, while in the other cases there is an improvement in the scores reaching 0.92, 0.95 and 0.94 (in order of appearance in the plot). But when the maximum depth gets a value higher than 5, the scores do not increase anymore because we are trying to fit the datasets in a more complex decision tree, overfitting it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python363jvsc74a57bd07624ca621d7ec95efb7e698cb955bbc1691c5a7ba16307613ed4f0fde6f6b148",
   "display_name": "Python 3.6.3 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "metadata": {
   "interpreter": {
    "hash": "7624ca621d7ec95efb7e698cb955bbc1691c5a7ba16307613ed4f0fde6f6b148"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}